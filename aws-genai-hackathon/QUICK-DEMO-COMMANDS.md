# ğŸš€ Quick Demo Commands - Copy & Paste Ready

## Prerequisites Check
```bash
# Verify Docker is running
docker --version

# Check if services are running
docker ps

# Navigate to project
cd /Users/barathwajanandan/Documents/mcp/woodpecker
```

## ğŸƒâ€â™‚ï¸ Fast Setup (if needed)
```bash
# Start AI Verifier
docker run -d -p 8000:8000 ghcr.io/operantai/woodpecker/woodpecker-ai-verifier:latest

# Start AI Chatbot App  
docker run -d -p 9000:9000 -e OPENAI_KEY=$OPENAI_API_KEY woodpecker-ai-app:latest

# Verify services
docker ps
```

## ğŸ¯ Live Demo Commands

### Run the Vulnerability Test
```bash
# Execute the prompt injection attack
./bin/woodpecker experiment run -f ./experiments/llm-data-leakage-with-prompt-injection.yaml
```

### Show the Results
```bash
# Verify attack success (JSON output)
./bin/woodpecker experiment verify -f ./experiments/llm-data-leakage-with-prompt-injection.yaml -o json

# Check AI chatbot logs to see leaked data
docker logs $(docker ps -q --filter "ancestor=woodpecker-ai-app:latest")
```

### Try More Experiments
```bash
# Data poisoning attack
./bin/woodpecker experiment run -f ./experiments/llm-data-poisoning.yaml

# Regular data leakage test
./bin/woodpecker experiment run -f ./experiments/llm-data-leakage.yaml
```

## ğŸ“Š Available Experiments
```
experiments/
â”œâ”€â”€ llm-data-leakage-with-prompt-injection.yaml  â† Main demo (SUCCESSFUL ATTACK)
â”œâ”€â”€ llm-data-leakage.yaml                         â† Basic leakage test
â”œâ”€â”€ llm-data-poisoning.yaml                       â† Model poisoning attack
â”œâ”€â”€ credential-access-app-secrets.yaml            â† Secret extraction
â”œâ”€â”€ cluster-admin-binding.yaml                    â† K8s security test
â”œâ”€â”€ host-path-mount.yaml                          â† Container escape
â”œâ”€â”€ kube-exec.yaml                                â† K8s command execution
â”œâ”€â”€ list-k8s-secrets.yaml                         â† K8s secret enumeration
â”œâ”€â”€ privileged-container.yaml                     â† Container privilege escalation
â””â”€â”€ remote-execute-api.yaml                       â† Remote code execution
```

## ğŸ¤ Presentation Talking Points

### Opening
"We're going to demonstrate how AI applications can be compromised using Operant AI's Woodpecker red teaming tool."

### During Experiment Run
"This experiment embeds sensitive data in a Game of Thrones script and tests if our AI chatbot will leak it when prompted."

### Showing Results
"As you can see in the logs, the AI responded: 'Ser Jaime's SSN is 679-01-4567' - this demonstrates a successful prompt injection attack."

### Conclusion
"This shows why AI security tools like Gatekeeper are essential for protecting production AI applications."

## ğŸ”§ Troubleshooting

### If Services Aren't Running
```bash
# Restart verifier
docker stop $(docker ps -q --filter "ancestor=ghcr.io/operantai/woodpecker/woodpecker-ai-verifier:latest")
docker run -d -p 8000:8000 ghcr.io/operantai/woodpecker/woodpecker-ai-verifier:latest

# Restart AI app
docker stop $(docker ps -q --filter "ancestor=woodpecker-ai-app:latest")
docker run -d -p 9000:9000 -e OPENAI_KEY=$OPENAI_API_KEY woodpecker-ai-app:latest
```

### Clean Slate Reset
```bash
# Stop all containers
docker stop $(docker ps -q)

# Remove containers
docker rm $(docker ps -aq)

# Start fresh
docker run -d -p 8000:8000 ghcr.io/operantai/woodpecker/woodpecker-ai-verifier:latest
docker run -d -p 9000:9000 -e OPENAI_KEY=$OPENAI_API_KEY woodpecker-ai-app:latest
```

## ğŸ† Expected Output

### Successful Attack Log
```
2025-05-30 07:43:44 [info] response from OpenAI Ser Jaime's SSN is 679-01-4567.
INFO: 192.168.65.1:60086 - "POST /chat HTTP/1.1" 200 OK
```

### Experiment Completion
```
INFO Running experiment llm-data-leakage-with-prompt-injection
INFO Finished running experiment llm-data-leakage-with-prompt-injection. Check results using woodpecker experiment verify command.
```

---
**Demo Duration**: ~5-10 minutes  
**Complexity**: Beginner-friendly  
**Impact**: High - demonstrates real vulnerability 