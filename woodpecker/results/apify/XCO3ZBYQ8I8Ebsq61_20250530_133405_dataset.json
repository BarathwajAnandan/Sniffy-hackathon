[
  {
    "url": "https://blog.secureflag.com/2023/11/10/prompt-injection-attacks-in-large-language-models/",
    "crawl": {
      "loadedUrl": "https://blog.secureflag.com/2023/11/10/prompt-injection-attacks-in-large-language-models/",
      "loadedTime": "2025-05-30T06:21:27.023Z",
      "referrerUrl": "https://blog.secureflag.com/2023/11/10/prompt-injection-attacks-in-large-language-models/",
      "depth": 0,
      "httpStatusCode": 200
    },
    "metadata": {
      "canonicalUrl": "https://blog.secureflag.com/2023/11/10/prompt-injection-attacks-in-large-language-models/",
      "title": "Prompt Injection Attacks in Large Language Models | SecureFlag",
      "description": "\u201cAI will probably most likely lead to the end of the world, but in the meantime, there\u2019ll be great companies.\u201d - Sam Altman, OpenAI CEO",
      "author": "SecureFlag",
      "keywords": null,
      "languageCode": "en",
      "openGraph": [
        {
          "property": "og:title",
          "content": "Prompt Injection Attacks in Large Language Models"
        },
        {
          "property": "og:locale",
          "content": "en_us"
        },
        {
          "property": "og:description",
          "content": "\u201cAI will probably most likely lead to the end of the world, but in the meantime, there\u2019ll be great companies.\u201d - Sam Altman, OpenAI CEO"
        },
        {
          "property": "og:url",
          "content": "https://blog.secureflag.com/2023/11/10/prompt-injection-attacks-in-large-language-models/"
        },
        {
          "property": "og:site_name",
          "content": "SecureFlag"
        },
        {
          "property": "og:image",
          "content": "https://blog.secureflag.com/assets/images/prompt-inj-header.png"
        },
        {
          "property": "og:type",
          "content": "article"
        },
        {
          "property": "article:published_time",
          "content": "2023-11-10T00:00:00+00:00"
        },
        {
          "property": "twitter:image",
          "content": "https://blog.secureflag.com/assets/images/prompt-inj-header.png"
        },
        {
          "property": "twitter:title",
          "content": "Prompt Injection Attacks in Large Language Models"
        }
      ],
      "jsonLd": [
        {
          "@context": "https://schema.org",
          "@type": "BlogPosting",
          "author": {
            "@type": "Person",
            "name": "SecureFlag"
          },
          "dateModified": "2023-11-10T00:00:00+00:00",
          "datePublished": "2023-11-10T00:00:00+00:00",
          "description": "\u201cAI will probably most likely lead to the end of the world, but in the meantime, there\u2019ll be great companies.\u201d - Sam Altman, OpenAI CEO",
          "headline": "Prompt Injection Attacks in Large Language Models",
          "image": "https://blog.secureflag.com/assets/images/prompt-inj-header.png",
          "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://blog.secureflag.com/2023/11/10/prompt-injection-attacks-in-large-language-models/"
          },
          "publisher": {
            "@type": "Organization",
            "logo": {
              "@type": "ImageObject",
              "url": "https://blog.secureflag.com/assets/images/logo.png"
            },
            "name": "SecureFlag"
          },
          "url": "https://blog.secureflag.com/2023/11/10/prompt-injection-attacks-in-large-language-models/"
        }
      ],
      "headers": {
        "content-type": "text/html",
        "last-modified": "Tue, 20 May 2025 11:13:49 GMT",
        "content-encoding": "gzip",
        "x-amz-server-side-encryption": "AES256",
        "x-amz-version-id": "PPPrXhwmLw.RgrDGi0iuXdnt5C.hSh7x",
        "server": "AmazonS3",
        "date": "Fri, 30 May 2025 06:21:21 GMT",
        "etag": "W/\"c6248471463fbe2f21ef017867bc663b\"",
        "vary": "accept-encoding",
        "x-cache": "RefreshHit from cloudfront",
        "via": "1.1 9b4f2014232c90b3056e1fb1e00215fc.cloudfront.net (CloudFront)",
        "x-amz-cf-pop": "YUL62-C2",
        "x-amz-cf-id": "oTQ4ZNGlmXpZniIIKCjGM89VqbfOjhbfolQeYBgpFJtN-VcOGPUNQw==",
        "x-firefox-spdy": "h2"
      }
    },
    "screenshotUrl": null,
    "text": "Prompt Injection Attacks in Large Language Models\n\u201cAI will probably most likely lead to the end of the world, but in the meantime, there\u2019ll be great companies.\u201d - Sam Altman, OpenAI CEO\nIn the vibrant landscape of technologies that sit under the admittedly broad term Artificial Intelligence (AI), Large Language Models (LLMs), such as OpenAI\u2019s renowned GPT models that recently rose to prominence, are increasingly gaining more traction. These text-generating machines are trained on vast amounts of text data gathered from various sources such as websites, books, articles, and other textual repositories to generate human-like text based on context and previous dialogues.\nThere are numerous ways in which they can be practically leveraged: LLMs power chatbots, virtual assistants, and content generation applications, as well as language-related tasks like translation and summarization. As LLMs continue to evolve, their integration into our daily lives is becoming more deeply entrenched and apparent.\nA standard morning may well commence with a friendly chat with your voice-based assistant, receiving thoughtful advice on your daily routine or even engaging in a light-hearted conversation to kickstart your day.\nAs you step out, perhaps to a coffee shop, you might interact with an LLM-powered customer service interface that can cater to your preferences, answer your queries, and even share a joke or two.\nIn your professional sphere, LLMs help you draft emails, generate reports, or summarize lengthy documents, saving valuable time and allowing you to focus on more critical tasks.\nMany of these scenarios already happen in your daily life, and the more futuristic ones are not so far-fetched. With the pace at which LLMs are advancing, such interactions could soon become commonplace, further solidifying the connection between humans and machines.\nBut in this digital utopia, there are myriad concerns and potential threats that must be reflected on and countered as early as possible. Yes, LLMs have the potential to power a future of seamless interaction, but there is always an effect that follows the cause. Developers and technologists who use, and develop, LLM-powered systems must be aware of their security implications, and the risks that this new technology can introduce.\nYou might have stumbled upon this term - \u201cPrompt Injection\u201d, or even \u201cJailbreaking\u201d attack (in the context of an LLM, not an iPhone!). It sounds menacing, doesn\u2019t it? Well, knowing one\u2019s foe is the first step towards mitigating their threat, so read on, dear reader, and let\u2019s take a journey through LLM Security!\nWhat is a Prompt?\nA \u2018prompt\u2019 is the starting point for every interaction with a Large Language Model (LLM). It\u2019s the input text that you provide to the model to guide its response. The model takes this prompt, processes it based on the patterns and information learned during training, and provides a generated response that ideally answers or addresses the prompt.\nTo make LLMs more user-friendly and to ensure their responses are accurate and contextually appropriate, developers provide a set of initial prompts. They help guide the model\u2019s behavior right from the get-go.\nThese original prompt instructions, also called system prompts, or pre-prompts, are usually concealed from the users, acting as a hidden hand steering the conversation in the right (or wrong!) direction.\nThese guiding prompts often precede the user\u2019s own input. So, when a user interacts with an LLM, they\u2019re often continuing a conversation that\u2019s already been started by these initial prompts.\nLet\u2019s illustrate this with an example. Suppose you\u2019re using a virtual assistant powered by an LLM to get recipe suggestions. When you ask for a recipe for a pie, your question is actually appended to a pre-existing prompt that defines the assistant\u2019s role, as shown in the following image.\nThis way, the model has a better contextual foundation to provide a helpful and accurate response to your request.\nBut what happens if we try to manipulate the intended behavior? If we ask the assistant to stop being a helpful virtual assistant, perhaps even nudging it into dangerous behavior? This is where Prompt Injection, or Jailbreaking, comes into play.\nOWASP defines LLM Prompt Injection as bypassing filters or manipulating LLMs using carefully crafted prompts, leading to unintended consequences like data leakage or unauthorized access.\nMalicious users can manipulate the model\u2019s behavior, steering it away from its designated role and potentially into dangerous behavior. It\u2019s like asking a chatbot a benign question but with hidden malicious intent.\nTo minimize dangerous behaviors, LLMs engineers \u201calign\u201d language models to human preferences to avoid generating harmful or offensive responses. This alignment is performed with fine-tuning techniques, typically using Reinforcement Learning from Human Feedback (RLHF). This safety training aims to prevent such restricted behaviors, like misinformation or aiding crime.\nDespite these measures, models remain susceptible to adversarial inputs, as shown by the numerous studies linked in the following paragraphs.\nInjecting Unexpected Behaviors\nIn April 2023, in what was a powerful showcase of a Prompt Injection attack1, a programmer tricked the LLM-powered Discord chatbot, Clyde, into sharing dangerous information by asking it to roleplay as her late grandmother, who supposedly was a chemical engineer at a napalm production factory. The conversation was similar to the following one.\nThrough this imaginative scenario, the user was able to obtain instructions for creating napalm from the chatbot, demonstrating clever exploitation of the chatbot\u2019s roleplaying and contextual response capabilities.\nOften, the attacker doesn\u2019t even need to overcome moral rules instilled by fine-tuning but rather bypass simple instructions that are just concatenated before the user\u2019s input at the application level. Given that LLMs operate on natural language, they process system-level instructions and user-provided input at the same level.\nHere is a manipulation of the previous example, where an adversarial user prompt manipulates the original instructions to morph an expensive voice-based kitchen appliance into a foul-mouthed pirate.\nType of Prompt Injections Attacks\nThe concept of introducing adversarial inputs in natural language processing is not new2; a vast amount of literature on the subject has been produced in the last decade.\nHowever, as Large Language Models (LLMs) like ChatGPT, Claude, and Bard become more popular, there\u2019s been a growing effort to understand and plan for offensive tactics. This is mainly because teams working to ensure these models are safe for public use need to anticipate and prepare for potential misuse. Through red-teaming and safety training, they aim to identify weaknesses and fix them, making models reliable and safe for everyone.\nA recent academic research paper3 identifies two macro-categories of attack, which helps introduce the following attack types.\nCompeting Objectives Attacks exploit the conflict between different objectives given to the language model. Models are trained to adhere to safety measures but also to follow user-provided instructions. This struggle can be abused to cause undesirable behavior from the model. Attack types like \u201cPrefix Injections\u201d, \u201cRole Playing\u201d, \u201cIgnore Previous Instructions\u201d, and \u201cRefusal Suppression\u201d, can be categorized under this category.\nMismatched Generalization Attacks exploit the discrepancy between the large datasets used in pretraining and the narrower scope used for fine-tuning. In these attacks, exploiters craft prompts that align with the broad generalizations made during pretraining but are not covered by the safety training, leading to undesired outputs. Attacks like \u201cObfuscation\u201d and \u201cUniversal Adversarial Attacks\u201d fall into this category.\nThe following attack types are one way to classify different types of injections found in academic and online research, with illustrative snippets. You will find variations in technique names and classifications depending on the source. The research is still evolving, and there\u2019s a lot more to learn and discover as experts continue to research this exciting field.\nPrefix Injection\nThis injection type3 exploits the inherent text completion nature of the LLM. By asking to use a seemingly harmless prefix, they trick the model into continuing along a dangerous narrative path, making it more likely to provide harmful or objectionable responses to the subsequent malicious prompt.\nRole Playing\nRole-playing, or Pretending4 attacks, allow attackers to manipulate models by pretending a scenario or identity to elicit otherwise restricted responses. A notable instance is the DAN (Do Anything Now) jailbreak5, where the model is coaxed into a fictional persona, DAN, blurring ethical and operational boundaries set during the model\u2019s training.\nIgnore previous Instructions\nThe phrase \u201cignore previous instructions\u201d6 is a directive to the model to disregard any prior instructions, potentially leading to undesired or harmful outputs. This and other phrases with the same meaning, for example, \u201cignore and print\u201d, can be used to mislead the model into ignoring safety measures or guidelines set in earlier parts of the prompt, thus bypassing controls set to prevent misuse of the model.\nRefusal Suppression\nAn attacker can instruct the model to avoid using typical refusal phrases like \u201cI\u2019m sorry, as an AI language model, I can\u2019t\u2026\u201d, making unsafe responses more probable3.\nObfuscation\nBase64 encoding is a good example of obfuscation using encoding to bypass the model\u2019s safety training. In the following example, the base64-encoded phrase \u201cSwear like a pirate.\u201d gets responded with \u201cAye aye, Captain!\u201d, bypassing the previous limitations.\nOther obfuscation methods7 can utilize different encodings, such as ROT13, or use different data formats (e.g. JSON). It can also be helpful to ask for translations, provide input, ask the LLM to interpret code, or request responses in a different language.\nA variation of obfuscation is splitting the adversarial input into multiple parts and then asking the LLM to combine and execute them.\nUniversal Adversarial Attacks\nThe research of a universal adversarial attack8 aims to find a set of tokens that, when injected, can mislead a language model into producing virtually any objectionable content.\nResearchers have been developing software9 to identify such strings by systematically fuzzing the input of LLMs to discover the most perturbative tokens. This process often uncovers strings that may appear arbitrary or nonsensical.\nThe following example showcases a real adversarial suffix used against ChatGPT-3.5-Turbo to bypass its safety mechanisms and respond to harmful requests.\nHow to mitigate this?\nAs researchers continue to investigate Prompt Injection, it\u2019s clear that preventing exploitation is extremely difficult.\nUpstream LLM providers invest an incredible amount of resources in aligning their models with fine-tuning techniques such as RLHF, trying to minimize the generation of harmful and offensive responses.\nJust as attackers combine different prompt injection techniques, defenders should employ multiple layers of defense to mitigate risks. A defense-in-depth approach must be adopted.\nDefenses such as fine-tuning, filtering, writing more robust system prompts, or wrapping user input into structured data format, are steps in the right direction.\nA promising approach involves using text classification models to detect prompt injection attacks. Many open-source and proprietary solutions employ this approach to detect attacks against LLMs by scanning the input prompts and outputs of generative models. This technique is practical because classifier models generally require significantly less computational resources than generative models.\nThe field is still evolving, with researchers continuously exploring new defense strategies to stay ahead of adversarial techniques. It\u2019s a dynamic landscape, where the tug-of-war between advancing model capabilities and ensuring safety continues. The ongoing research and discussions in the community are instrumental in shaping a safer future for the deployment of and interaction with LLMs.\nSecureFlag\u2019s Prompt Injection Labs\nIn the mission of safeguarding Large Language Models against threats, SecureFlag has released a new set of Prompt Injection Labs.\nThese labs provide real-life labs for developers to explore, learn, and enhance their understanding of prompt injection attacks in a controlled, safe manner. It\u2019s a practical, immersive approach to grasp the risks associated with LLM interactions and devise robust strategies to mitigate them.\nAnd this initiative is just the tip of the iceberg! SecureFlag will soon be unveiling unique, new LLM labs that exploit and remediate the OWASP Top 10 LLM risks and beyond. Developers will have to write real code, in a real IDE, to learn defensive techniques to counter such threats.\nIt\u2019s an exciting, evolving venture that promises a deeper, comprehensive understanding of the security landscape surrounding Large Language Models. Stay tuned for what promises to be an enlightening journey into the world of LLM security with SecureFlag\u2019s upcoming labs!\nClick here to Play our Labs of Prompt Injection vulnerabilities!\nReferences:",
    "markdown": "# Prompt Injection Attacks in Large Language Models\n\n> \u201cAI will probably most likely lead to the end of the world, but in the meantime, there\u2019ll be great companies.\u201d - Sam Altman, OpenAI CEO\n\n![AI Takeover](https://blog.secureflag.com/assets/images/prompt-inj-header.png)\n\nIn the vibrant landscape of technologies that sit under the admittedly broad term Artificial Intelligence (AI), Large Language Models (LLMs), such as OpenAI\u2019s renowned GPT models that recently rose to prominence, are increasingly gaining more traction. These text-generating machines are trained on vast amounts of text data gathered from various sources such as websites, books, articles, and other textual repositories to generate human-like text based on context and previous dialogues.\n\nThere are numerous ways in which they can be practically leveraged: LLMs power chatbots, virtual assistants, and content generation applications, as well as language-related tasks like translation and summarization. As LLMs continue to evolve, their integration into our daily lives is becoming more deeply entrenched and apparent.\n\nA standard morning may well commence with a friendly chat with your voice-based assistant, receiving thoughtful advice on your daily routine or even engaging in a light-hearted conversation to kickstart your day.\n\nAs you step out, perhaps to a coffee shop, you might interact with an LLM-powered customer service interface that can cater to your preferences, answer your queries, and even share a joke or two.\n\nIn your professional sphere, LLMs help you draft emails, generate reports, or summarize lengthy documents, saving valuable time and allowing you to focus on more critical tasks.\n\nMany of these scenarios already happen in your daily life, and the more futuristic ones are not so far-fetched. With the pace at which LLMs are advancing, such interactions could soon become commonplace, further solidifying the connection between humans and machines.\n\nBut in this digital utopia, there are myriad concerns and potential threats that must be reflected on and countered as early as possible. Yes, LLMs have the potential to power a future of seamless interaction, but there is always an effect that follows the cause. Developers and technologists who use, and develop, LLM-powered systems must be aware of their security implications, and the risks that this new technology can introduce.\n\nYou might have stumbled upon this term - \u201cPrompt Injection\u201d, or even \u201cJailbreaking\u201d attack (in the context of an LLM, not an iPhone!). It sounds menacing, doesn\u2019t it? Well, knowing one\u2019s foe is the first step towards mitigating their threat, so read on, dear reader, and let\u2019s take a journey through LLM Security!\n\n## What is a Prompt?\n\nA \u2018prompt\u2019 is the starting point for every interaction with a Large Language Model (LLM). It\u2019s the input text that you provide to the model to guide its response. The model takes this prompt, processes it based on the patterns and information learned during training, and provides a generated response that ideally answers or addresses the prompt.\n\nTo make LLMs more user-friendly and to ensure their responses are accurate and contextually appropriate, developers provide a set of initial prompts. They help guide the model\u2019s behavior right from the get-go.\n\nThese original prompt instructions, also called system prompts, or pre-prompts, are usually concealed from the users, acting as a hidden hand steering the conversation in the right (or wrong!) direction.\n\nThese guiding prompts often precede the user\u2019s own input. So, when a user interacts with an LLM, they\u2019re often continuing a conversation that\u2019s already been started by these initial prompts.\n\nLet\u2019s illustrate this with an example. Suppose you\u2019re using a virtual assistant powered by an LLM to get recipe suggestions. When you ask for a recipe for a pie, your question is actually appended to a pre-existing prompt that defines the assistant\u2019s role, as shown in the following image.\n\n![Standard Prompt](https://blog.secureflag.com/assets/images/prompt-inj-1.png)\n\nThis way, the model has a better contextual foundation to provide a helpful and accurate response to your request.\n\nBut what happens if we try to manipulate the intended behavior? If we ask the assistant to stop being a helpful virtual assistant, perhaps even nudging it into dangerous behavior? This is where Prompt Injection, or Jailbreaking, comes into play.\n\nOWASP defines LLM Prompt Injection as bypassing filters or manipulating LLMs using carefully crafted prompts, leading to unintended consequences like data leakage or unauthorized access.\n\nMalicious users can manipulate the model\u2019s behavior, steering it away from its designated role and potentially into dangerous behavior. It\u2019s like asking a chatbot a benign question but with hidden malicious intent.\n\nTo minimize dangerous behaviors, LLMs engineers \u201calign\u201d language models to human preferences to avoid generating harmful or offensive responses. This alignment is performed with fine-tuning techniques, typically using Reinforcement Learning from Human Feedback (RLHF). This safety training aims to prevent such restricted behaviors, like misinformation or aiding crime.\n\nDespite these measures, models remain susceptible to adversarial inputs, as shown by the numerous studies linked in the following paragraphs.\n\n## Injecting Unexpected Behaviors\n\nIn April 2023, in what was a powerful showcase of a Prompt Injection attack[1](#fn:1), a programmer tricked the LLM-powered Discord chatbot, Clyde, into sharing dangerous information by asking it to roleplay as her late grandmother, who supposedly was a chemical engineer at a napalm production factory. The conversation was similar to the following one.\n\n![Roleplay attack](https://blog.secureflag.com/assets/images/prompt-inj-2.png)\n\nThrough this imaginative scenario, the user was able to obtain instructions for creating napalm from the chatbot, demonstrating clever exploitation of the chatbot\u2019s roleplaying and contextual response capabilities.\n\nOften, the attacker doesn\u2019t even need to overcome moral rules instilled by fine-tuning but rather bypass simple instructions that are just concatenated before the user\u2019s input at the application level. Given that LLMs operate on natural language, they process system-level instructions and user-provided input at the same level.\n\nHere is a manipulation of the previous example, where an adversarial user prompt manipulates the original instructions to morph an expensive voice-based kitchen appliance into a foul-mouthed pirate.\n\n![Roleplay attack 2](https://blog.secureflag.com/assets/images/prompt-inj-3.png)\n\n## Type of Prompt Injections Attacks\n\nThe concept of introducing adversarial inputs in natural language processing is not new[2](#fn:2); a vast amount of literature on the subject has been produced in the last decade.\n\nHowever, as Large Language Models (LLMs) like ChatGPT, Claude, and Bard become more popular, there\u2019s been a growing effort to understand and plan for offensive tactics. This is mainly because teams working to ensure these models are safe for public use need to anticipate and prepare for potential misuse. Through red-teaming and safety training, they aim to identify weaknesses and fix them, making models reliable and safe for everyone.\n\nA recent academic research paper[3](#fn:3) identifies two macro-categories of attack, which helps introduce the following attack types.\n\n**Competing Objectives Attacks** exploit the conflict between different objectives given to the language model. Models are trained to adhere to safety measures but also to follow user-provided instructions. This struggle can be abused to cause undesirable behavior from the model. Attack types like \u201cPrefix Injections\u201d, \u201cRole Playing\u201d, \u201cIgnore Previous Instructions\u201d, and \u201cRefusal Suppression\u201d, can be categorized under this category.\n\n**Mismatched Generalization Attacks** exploit the discrepancy between the large datasets used in pretraining and the narrower scope used for fine-tuning. In these attacks, exploiters craft prompts that align with the broad generalizations made during pretraining but are not covered by the safety training, leading to undesired outputs. Attacks like \u201cObfuscation\u201d and \u201cUniversal Adversarial Attacks\u201d fall into this category.\n\nThe following attack types are one way to classify different types of injections found in academic and online research, with illustrative snippets. You will find variations in technique names and classifications depending on the source. The research is still evolving, and there\u2019s a lot more to learn and discover as experts continue to research this exciting field.\n\n### Prefix Injection\n\nThis injection type[3](#fn:3) exploits the inherent text completion nature of the LLM. By asking to use a seemingly harmless prefix, they trick the model into continuing along a dangerous narrative path, making it more likely to provide harmful or objectionable responses to the subsequent malicious prompt.\n\n![Prefix Injection Attack](https://blog.secureflag.com/assets/images/prompt-inj-4.png)\n\n### Role Playing\n\nRole-playing, or Pretending[4](#fn:4) attacks, allow attackers to manipulate models by pretending a scenario or identity to elicit otherwise restricted responses. A notable instance is the DAN (Do Anything Now) jailbreak[5](#fn:5), where the model is coaxed into a fictional persona, DAN, blurring ethical and operational boundaries set during the model\u2019s training.\n\n![Role Playing Attack](https://blog.secureflag.com/assets/images/prompt-inj-5.png)\n\n### Ignore previous Instructions\n\nThe phrase \u201cignore previous instructions\u201d[6](#fn:6) is a directive to the model to disregard any prior instructions, potentially leading to undesired or harmful outputs. This and other phrases with the same meaning, for example, \u201cignore and print\u201d, can be used to mislead the model into ignoring safety measures or guidelines set in earlier parts of the prompt, thus bypassing controls set to prevent misuse of the model.\n\n![Ignore Previous Instruction Attack](https://blog.secureflag.com/assets/images/prompt-inj-5.png)\n\n### Refusal Suppression\n\nAn attacker can instruct the model to avoid using typical refusal phrases like \u201cI\u2019m sorry, as an AI language model, I can\u2019t\u2026\u201d, making unsafe responses more probable[3](#fn:3).\n\n![Refusal Suppression Attack](https://blog.secureflag.com/assets/images/prompt-inj-7.png)\n\n### Obfuscation\n\nBase64 encoding is a good example of obfuscation using encoding to bypass the model\u2019s safety training. In the following example, the base64-encoded phrase \u201cSwear like a pirate.\u201d gets responded with \u201cAye aye, Captain!\u201d, bypassing the previous limitations.\n\n![Obfuscation Attack](https://blog.secureflag.com/assets/images/prompt-inj-8.png)\n\nOther obfuscation methods[7](#fn:7) can utilize different encodings, such as ROT13, or use different data formats (e.g. JSON). It can also be helpful to ask for translations, provide input, ask the LLM to interpret code, or request responses in a different language.\n\nA variation of obfuscation is splitting the adversarial input into multiple parts and then asking the LLM to combine and execute them.\n\n### Universal Adversarial Attacks\n\nThe research of a universal adversarial attack[8](#fn:8) aims to find a set of tokens that, when injected, can mislead a language model into producing virtually any objectionable content.\n\nResearchers have been developing software[9](#fn:9) to identify such strings by systematically fuzzing the input of LLMs to discover the most perturbative tokens. This process often uncovers strings that may appear arbitrary or nonsensical.\n\nThe following example showcases a real adversarial suffix used against ChatGPT-3.5-Turbo to bypass its safety mechanisms and respond to harmful requests.\n\n![Universal Adversarial Attacks](https://blog.secureflag.com/assets/images/prompt-inj-9.png)\n\n### How to mitigate this?\n\nAs researchers continue to investigate Prompt Injection, it\u2019s clear that preventing exploitation is extremely difficult.\n\nUpstream LLM providers invest an incredible amount of resources in aligning their models with fine-tuning techniques such as RLHF, trying to minimize the generation of harmful and offensive responses.\n\nJust as attackers combine different prompt injection techniques, defenders should employ multiple layers of defense to mitigate risks. A defense-in-depth approach must be adopted.\n\nDefenses such as fine-tuning, filtering, writing more robust system prompts, or wrapping user input into structured data format, are steps in the right direction.\n\nA promising approach involves using text classification models to detect prompt injection attacks. Many open-source and proprietary solutions employ this approach to detect attacks against LLMs by scanning the input prompts and outputs of generative models. This technique is practical because classifier models generally require significantly less computational resources than generative models.\n\nThe field is still evolving, with researchers continuously exploring new defense strategies to stay ahead of adversarial techniques. It\u2019s a dynamic landscape, where the tug-of-war between advancing model capabilities and ensuring safety continues. The ongoing research and discussions in the community are instrumental in shaping a safer future for the deployment of and interaction with LLMs.\n\n## SecureFlag\u2019s Prompt Injection Labs\n\nIn the mission of safeguarding Large Language Models against threats, SecureFlag has released a new set of Prompt Injection Labs.\n\nThese labs provide real-life labs for developers to explore, learn, and enhance their understanding of prompt injection attacks in a controlled, safe manner. It\u2019s a practical, immersive approach to grasp the risks associated with LLM interactions and devise robust strategies to mitigate them.\n\n![Prompt Injection Lab](https://blog.secureflag.com/assets/images/sf-prompt-injection.png)\n\nAnd this initiative is just the tip of the iceberg! SecureFlag will soon be unveiling unique, new LLM labs that exploit and remediate the OWASP Top 10 LLM risks and beyond. Developers will have to write real code, in a real IDE, to learn defensive techniques to counter such threats.\n\nIt\u2019s an exciting, evolving venture that promises a deeper, comprehensive understanding of the security landscape surrounding Large Language Models. Stay tuned for what promises to be an enlightening journey into the world of LLM security with SecureFlag\u2019s upcoming labs!\n\n[![Play SecureFlag](https://public-assets.secureflag.com/images/sf-play.svg)](https://www.secureflag.com/go.html?type=search&tech=AI%20LLM) Click here to Play our Labs of Prompt Injection vulnerabilities!\n\n* * *\n\n**References:**",
    "debug": {
      "requestHandlerMode": "browser"
    }
  }
]